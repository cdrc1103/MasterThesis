{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adapted from\n",
    "# https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Huggingface transformers\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n",
    "\n",
    "# Then what you need from tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Libraries to import and process the data set\n",
    "from Utilities import directories\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 10\n"
     ]
    }
   ],
   "source": [
    "### --------- Load and transform data ---------- ###\n",
    "with open(directories.data + \"/example_extract.txt\", \"rb\") as file:   # Unpickling\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Transfer data to pandas ds and set model output (classes) as categorical\n",
    "data_df = pd.DataFrame(data, index=['Description', 'CPC_label']).transpose()\n",
    "data_df['CPC_label'] = pd.Categorical(data_df['CPC_label'])\n",
    "# Transform output to numeric\n",
    "data_df['CPC_code'] = data_df['CPC_label'].cat.codes\n",
    "# Drop rows with classes that only occur once\n",
    "data_df.CPC_code.value_counts()\n",
    "# Split into train and test\n",
    "train, test = train_test_split(data_df, test_size = 0.2)\n",
    "print(len(train), len(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### --------- Setup BERT ---------- ###\n",
    "# Name of the BERT model to use\n",
    "model_name = 'bert-base-uncased'\n",
    "# Max length of tokens\n",
    "max_length = 100\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "# from_pretrained loads weights from pretrained model\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
    "# Load the Transformers BERT model\n",
    "transformer_model = TFBertModel.from_pretrained(model_name, config = config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vie default configurations\n",
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "bert (TFBertMainLayer)       TFBaseModelOutputWithPool 109482240 \n",
      "_________________________________________________________________\n",
      "pooled_output (Dropout)      (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "cpc (Dense)                  (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 109,494,544\n",
      "Trainable params: 109,494,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### ------- Build the model ------- ###\n",
    "# Load the MainLayer\n",
    "bert = transformer_model.layers[0]\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "# Then build your model output\n",
    "cpc_class = Dense(units=len(data_df.CPC_code.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='cpc')(pooled_output)\n",
    "outputs = {'cpc': cpc_class}\n",
    "\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.8358 - accuracy: 0.0526 - val_loss: 2.6805 - val_accuracy: 0.1000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7080 - accuracy: 0.0000e+00 - val_loss: 2.5176 - val_accuracy: 0.2000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4685 - accuracy: 0.1053 - val_loss: 2.3797 - val_accuracy: 0.4000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3680 - accuracy: 0.5000 - val_loss: 2.2693 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2559 - accuracy: 0.4474 - val_loss: 2.2116 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1678 - accuracy: 0.4474 - val_loss: 2.1446 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0669 - accuracy: 0.4474 - val_loss: 2.1021 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0320 - accuracy: 0.4474 - val_loss: 2.0636 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9606 - accuracy: 0.4474 - val_loss: 2.0079 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9140 - accuracy: 0.4474 - val_loss: 1.9305 - val_accuracy: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "### ------- Train the model ------- ###\n",
    "# Set an optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05,\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = {'cpc': CategoricalCrossentropy(from_logits = True)}\n",
    "metric = {'cpc': CategoricalAccuracy('accuracy')}# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss,\n",
    "    metrics = metric)\n",
    "\n",
    "# Ready output data for the model\n",
    "y_cpc = to_categorical(data_df['CPC_code'])\n",
    "\n",
    "# Tokenize the input (takes some time)\n",
    "x = tokenizer(\n",
    "    text=data_df['Description'].to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = False,\n",
    "    verbose = True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x={'input_ids': x['input_ids']},\n",
    "    y={'cpc': y_cpc},\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}