{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from Utilities.directories import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define directories to load raw data from and to save serialized data to\n",
    "root_dir = pathlib.Path(data + \"/example_extrect_ma\")\n",
    "save_dir = pathlib.Path(data + \"/example_extract-tfecord\")\n",
    "dataset = tf.data.Dataset.list_files(str(root_dir/'*'), seed=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes_list {\n",
      "  value: \"some string\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(_bytes_feature(b'some string'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "\n",
    "    feature = {\n",
    "        \"feature0\": _bytes_feature(feature0),\n",
    "        \"feature1\": _int64_feature(feature1)\n",
    "    }\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "b'\\n)\\n\\x14\\n\\x08feature0\\x12\\x08\\n\\x06\\n\\x04test\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x0f'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the serialization\n",
    "\n",
    "serialized_example = serialize_example(feature0=b'test', feature1=15)\n",
    "serialized_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"Adapted from: https://gist.github.com/dschwertfeger/3288e8e1a2d189e5565cc43bb04169a1\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Utilities.directories import lexis_data, lexis_abstract\n",
    "import pathlib\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import glob\n",
    "\n",
    "_SEED = 2020\n",
    "_COMPRESSION_SCALING_FACTOR = 4\n",
    "_COMPRESSION_LIB = \"ZLIB\" # 'ZLIB is the coompression type\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _parallelize(func, data):\n",
    "    processes = cpu_count() - 1\n",
    "    with Pool(processes) as pool:\n",
    "        # We need the enclosing list statement to wait for the iterator to end\n",
    "        # https://stackoverflow.com/a/45276885/1663506\n",
    "        list(tqdm(pool.imap_unordered(func, data), total=len(data)))\n",
    "\n",
    "class TFREcordsConverter:\n",
    "    \"\"\"Convert XML files to TFRecords.\"\"\"\n",
    "\n",
    "    # When compression is used, resulting TFRecord files are four to five times\n",
    "    # smaller. So, we can reduce the number of shards by this factor\n",
    "    _COMPRESSION_SCALING_FACTOR = 4\n",
    "\n",
    "    def __init__(self, filepaths, output_dir, test_size, val_size):\n",
    "        \"\"\"\n",
    "\n",
    "        :param filepaths: pandas dataframe with filepaths\n",
    "        :param output_dir:\n",
    "        :param test_size:\n",
    "        :param val_size:\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Shuffle data by \"sampling\" the entire data frame\n",
    "        self.filepaths = filepaths.sample(frac=1, random_state=_SEED)\n",
    "\n",
    "        # Calculate number of instances for each sub dataset\n",
    "        n_samples = len(filepaths)\n",
    "        self.n_test = math.ceil(test_size * n_samples)\n",
    "        self.n_val = math.ceil(val_size * n_samples)\n",
    "        self.n_train = n_samples - self.n_test - self.n_val\n",
    "\n",
    "        # Determine number of shards per sub dataset\n",
    "        self.n_shards_test = self._n_shards(self.n_test)\n",
    "        self.n_shards_val = self._n_shards(self.n_val)\n",
    "        self.n_shards_train = self._n_shards(self.n_train)\n",
    "\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('{}.{}(output_dir={}, n_shards_train={}, n_shards_test={}, '\n",
    "                'n_shards_val={}, n_train={}, '\n",
    "                'n_test={}, n_val={})').format(\n",
    "            self.__class__.__module__,\n",
    "            self.__class__.__name__,\n",
    "            self.output_dir,\n",
    "            self.n_shards_train,\n",
    "            self.n_shards_test,\n",
    "            self.n_shards_val,\n",
    "            self.n_train,\n",
    "            self.n_test,\n",
    "            self.n_val,\n",
    "        )\n",
    "\n",
    "    def _n_shards(self, n_samples):\n",
    "        \"\"\"\n",
    "        Compute number of shards for number of samples.\n",
    "\n",
    "        TFRecords are split into multiple shards. Each shard's size should be\n",
    "        between 100 MB and 200 MB according to the TensorFlow documentation.\n",
    "\n",
    "        :param\n",
    "        n_samples: int\n",
    "            The number of samples to split into TFRecord shards.\n",
    "        :return:\n",
    "        n_shards: int\n",
    "            The number of shards needed to fit the provided number of samples.\n",
    "        \"\"\"\n",
    "\n",
    "        shard_size = 2 * 10**8\n",
    "        avg_file_size = 1.6 * 10**5 # rough estimation since the file size per document varies a lot.\n",
    "        files_per_shard = math.ceil(shard_size / avg_file_size) * _COMPRESSION_SCALING_FACTOR\n",
    "        return math.ceil(n_samples / files_per_shard)\n",
    "\n",
    "    def _process_files(self, shard_data):\n",
    "        \"\"\"\n",
    "        Write TFRecord file.\n",
    "\n",
    "        :param\n",
    "        shard_data: tuple(str, list)\n",
    "            A tup√∂e containing the shard path and the list of indices to write to it.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        shard_path, indices = shard_data\n",
    "        with tf.io.TFRecordWriter(shard_path, options=_COMPRESSION_LIB):\n",
    "            for index in indices:\n",
    "                file_path = self.filepaths.iloc[index,0] # get the respective filepath\n",
    "                xml_tree = self._parse_xml(file_path)\n",
    "\n",
    "                # Extract features\n",
    "                # abstract\n",
    "                abstract = xml_tree[\"lexisnexis-patent-document\"][\"abstract\"]\n",
    "                if type(abstract) == list:\n",
    "                    for element in abstract:\n",
    "                        if element[\"@lang\"] == \"eng\":\n",
    "                            abstract_text = element[\"p\"]\n",
    "                            break\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'abstract': _bytes_feature(abstract_text),\n",
    "                    'label': _bytes_feature(),\n",
    "\n",
    "                }))\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_xml(path):\n",
    "        \"\"\"\n",
    "        Load all xml files from the directory and convert them to a list of\n",
    "        python dictionaries.\n",
    "\n",
    "        :param file_dir: Directory to the files\n",
    "        :return: list of python dictionaries\n",
    "        \"\"\"\n",
    "\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "        xml_str = ET.tostring(root)\n",
    "        xml_tree = xmltodict.parse(xml_str)\n",
    "\n",
    "        return xml_tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\MLData\\thesis\\Datasets\\example_extrect_ma\n",
      "168785.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(root_dir)\n",
    "df = pd.read_csv(pathlib.Path.joinpath(root_dir, \"paths.csv\"))\n",
    "n_rows = len(df)\n",
    "avg_size = 0\n",
    "for row in df.iloc[:,0]:\n",
    "    avg_size += os.path.getsize(row)\n",
    "avg_size /= n_rows\n",
    "print(avg_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}